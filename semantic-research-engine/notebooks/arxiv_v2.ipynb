{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install git+https://github.com/openai/swarm.git\n",
    "# pip install langchain langchain_community langchain_ollama\n",
    "# pip install arxiv2text\n",
    "# pip install arxiv\n",
    "# pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "model = os.getenv('LLM_MODEL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from swarm import Swarm, Agent\n",
    "\n",
    "ollama_client = OpenAI(\n",
    "    base_url='http://localhost:11434/v1',\n",
    "    api_key='ollama'  # required but unused\n",
    ")\n",
    "#\n",
    "client= Swarm(client=ollama_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "import pandas as pd\n",
    "\n",
    "def get_url_topic(topic):\n",
    "    # Prompt user for the topic to search\n",
    "    print(topic)\n",
    "    #topic = \"ChunkRag\"\n",
    "    # Set up the search parameters\n",
    "    search = arxiv.Search(\n",
    "        query=topic,\n",
    "        max_results=1,  # You can adjust this number as needed\n",
    "        sort_by=arxiv.SortCriterion.SubmittedDate,\n",
    "        sort_order=arxiv.SortOrder.Descending,\n",
    "    )\n",
    "\n",
    "    # Prepare a list to store results\n",
    "    all_data = []\n",
    "\n",
    "    # Execute the search and collect results\n",
    "    for result in search.results():\n",
    "        #print(result)\n",
    "        paper_info = {\n",
    "            \"Title\": result.title,\n",
    "            \"Date\": result.published.date(),\n",
    "            \"Id\": result.entry_id,\n",
    "            \"Summary\": result.summary,\n",
    "            \"URL\": result.pdf_url,\n",
    "        }\n",
    "        all_data.append(paper_info)\n",
    "\n",
    "  \n",
    "    if all_data:\n",
    "        results = \"\\n\\n\".join([f\"Title:{d['Title']}\\nDate:{d['Date']}\\nURL:{d['URL']}\\nSummary:{d['Summary']}\"for d in all_data])\n",
    "        #print(results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkRag\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6409/2044149504.py:20: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  for result in search.results():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title:ChunkRAG: Novel LLM-Chunk Filtering Method for RAG Systems\n",
      "Date:2024-10-25\n",
      "URL:http://arxiv.org/pdf/2410.19572v4\n",
      "Summary:Retrieval-Augmented Generation (RAG) systems using large language models\n",
      "(LLMs) often generate inaccurate responses due to the retrieval of irrelevant\n",
      "or loosely related information. Existing methods, which operate at the document\n",
      "level, fail to effectively filter out such content. We propose LLM-driven chunk\n",
      "filtering, ChunkRAG, a framework that enhances RAG systems by evaluating and\n",
      "filtering retrieved information at the chunk level. Our approach employs\n",
      "semantic chunking to divide documents into coherent sections and utilizes\n",
      "LLM-based relevance scoring to assess each chunk's alignment with the user's\n",
      "query. By filtering out less pertinent chunks before the generation phase, we\n",
      "significantly reduce hallucinations and improve factual accuracy. Experiments\n",
      "show that our method outperforms existing RAG models, achieving higher accuracy\n",
      "on tasks requiring precise information retrieval. This advancement enhances the\n",
      "reliability of RAG systems, making them particularly beneficial for\n",
      "applications like fact-checking and multi-hop reasoning.\n"
     ]
    }
   ],
   "source": [
    "print(get_url_topic(\"ChunkRag\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "from langchain_ollama import ChatOllama\n",
    "from arxiv2text import arxiv_to_text\n",
    "from openai import OpenAI\n",
    "\n",
    "ollama_client = OpenAI(\n",
    "    base_url='http://localhost:11434/v1',\n",
    "    api_key='ollama'  # required but unused\n",
    ")\n",
    "\n",
    "class SummarizerAgent:\n",
    "    def __init__(self):\n",
    "        self.llm = ChatOllama(model=\"llama3.2:1b\",\n",
    "                              temperature=0.0,\n",
    "                              num_predict=1000)\n",
    "    \n",
    "    def extract_content(self,url:str) -> str:\n",
    "        # Replace with your specific arXiv PDF URL\n",
    "        pdf_url = url\n",
    "        extracted_text = arxiv_to_text(pdf_url)\n",
    "        return extracted_text\n",
    "\n",
    "    def summarize_paper(self, paper: Dict,content: str) -> str:\n",
    "        \"\"\"\n",
    "        Summarize a single paper using Llama2\n",
    "        \"\"\"\n",
    "        prompt = f\"\"\"\n",
    "        Please provide a concise summary of the following research paper:\n",
    "        Title: {paper['title']}\n",
    "        Authors: {', '.join(paper['authors'])}\n",
    "        Abstract: {paper['summary']}\n",
    "        Content : {content}\n",
    "        \n",
    "        Generate a clear ,concise and informative summary in no more than 6-8 sentences.\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.llm.predict(prompt)\n",
    "\n",
    "    def summarize_papers(self, papers: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Summarize multiple papers\n",
    "        \"\"\"\n",
    "        summarized_papers = []\n",
    "        for paper in papers:\n",
    "            summary = self.summarize_paper(paper)\n",
    "            summarized_papers.append({\n",
    "                'title': paper['title'],\n",
    "                'summary': summary,\n",
    "                'original_paper': paper\n",
    "            })\n",
    "        \n",
    "        return summarized_papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "llm =ChatOllama(model=\"llama3.2:1b\",\n",
    "                              temperature=0.0,\n",
    "                              num_predict=1000)\n",
    "#\n",
    "def extract_content(url):\n",
    "    summ = SummarizerAgent()\n",
    "    content = summ.extract_content(url)\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = extract_content(\"http://arxiv.org/pdf/2410.19572v3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_agent = Agent(name=\"Extract URL Assistant\",\n",
    "                  instruction=\"Get the arxiv search results for the given topic.\",\n",
    "                  functions=[get_url_topic],\n",
    "                  model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_url = Agent(name=\"URL Assistant\",\n",
    "                  instruction=\"Get the URL from the given content.\",\n",
    "                  model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_agent = Agent(name=\"Extract Summary Assistant\",\n",
    "                  instruction=\"\"\"Generate a clear ,concise and informative summary of the arxiv paper.The Summary should include the authors of the paper , the date it was published and \n",
    "                  the concept behind the topic explained i the paper.\"\"\",\n",
    "                  functions=[extract_content],\n",
    "                  model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Summary Report: System Development and Research\n",
      "\n",
      "#### **Introduction**\n",
      "The report summarizes the ongoing research and development work concerning various natural language processing (NLP) tasks, particularly those involving retrieval-augmented generation methods. These methods have been instrumental in enhancing information retrieval performance by integrating external knowledge sources back into downstream AI models.\n",
      "\n",
      "#### **Research Methodology**\n",
      "\n",
      "1. **Literature Review**: \n",
      "   - Comprehensive reviews of existing literature on self-reflective retrieval augmentation (self-reflective rag), fine-grained document retrieval for fact-checking tasks, multi-hop reasoning using graph-based methodologies, and corrective retrieval augmentation.\n",
      "   - Detailed analyses focused on critical aspects such as error propagation, computational efficiency, and scalability.\n",
      "\n",
      "2. **Experimental Setup**: \n",
      "   - Utilized benchmarks like RERAC (Retrieval-Augmented Generation), MetaRAG (Multi-meta-rag), Cove65B (Improving factual accuracy through iterative engineering) for model evaluation.\n",
      "   - Experiment design with controlled conditions to isolate variables affecting retrieval and generation performance.\n",
      "\n",
      "#### **Key Findings**\n",
      "\n",
      "1. **Self-Reflective Retrieval-Augmented Generation**: \n",
      "   - Asai et al.'s self-reflective rag provided improved knowledge-intensive tasks by enabling models to critically evaluate their outputs, leading to more accurate results.\n",
      "   - Challenges include the need for comprehensive embeddings and the risk of creating irrelevant responses due to primary division errors.\n",
      "\n",
      "2. **Fine-Grained Document Retrieval**: \n",
      "   - Rony et al.'s fine-grained document retrieval demonstrated significant improvements in fact-checking by enabling detailed checks on document accuracy.\n",
      "   - Issues such as high method costs during scaling further necessitate optimized computational strategies.\n",
      "\n",
      "3. **Multi-Hop Reasoning with Graph-Based Retrievals**: \n",
      "   - Bhakthavatsalam et al.'s multi-hop reasoning models indicated potential insights from graph-based methodologies in enhancing retrieval performance for complex queries.\n",
      "   - However, its applicability was still not thoroughly validated across diverse domains due to resource constraints.\n",
      "\n",
      "4. **Retrieval-Augmented Corrective Generation (CRAG)**: \n",
      "   - Your et al.'s CRAG addressed the challenge of iterative error correction without compromising model generality.\n",
      "   - CRAG showed promising results in various NLP tasks but lacked extensive cross-validation studies for multiple domains and long-form generation.\n",
      "\n",
      "#### **Future Research Directions**\n",
      "\n",
      "1. **Optimized Computational Efficiency**: Develop strategies to reduce method overhead, emphasizing fine-tuning techniques like instruction tuning for large language models (LLM) as a potential area of study.\n",
      "\n",
      "2. **Cross-Domain Validation**: Further validate the efficacy of these methods in diverse, real-world applications through comprehensive benchmark studies that encompass multiple domains and tasks.\n",
      "\n",
      "3. **Scalability and Resilience**: Investigate approaches to improve scalability so that models perform efficiently at large scales without compromising performance metrics like factual consistency or relevancy assessment accuracy.\n",
      "\n",
      "4. **Multi-hop Reasoning Integration**: Further investigate how graph-based methods can be integrated into multi-hop reasoning scenarios, potentially addressing current limitations related to error propagation and computational costs.\n",
      "\n",
      "#### **References**\n",
      "\n",
      "- [List of all the references mentioned in the report]\n",
      "\n",
      "This comprehensive summary provides a structured overview of the research landscape surrounding retrieval-augmented generation techniques. The key findings offer insights that are crucial for refining these methods to meet real-world needs while maintaining their effectiveness and robustness across different application scenarios.\n"
     ]
    }
   ],
   "source": [
    "summary_response = client.run(\n",
    "    agent=content_agent,\n",
    "    messages=[{\"role\": \"user\", \"content\": content }],# retrive paper content\n",
    ")\n",
    "print(summary_response.messages[-1][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def run_arxiv_paper_summary_workflow(topic):\n",
    "    #Step1 Get the arxiv search results\n",
    "    paper_details_response = client.run(agent=url_agent,\n",
    "           messages=[{\"role\":\"user\",\"content\":f\"Get me the details for {topic}\"}])\n",
    "    text = paper_details_response.messages[-1]['content']\n",
    "    print(text)\n",
    "    # Step 2 Extract the URL from the search results\n",
    "    url_response = client.run(agent=extract_url,\n",
    "    messages=[{\"role\":\"user\",\"content\":f\"Get me the URL from the content{text}\"}])\n",
    "    #\n",
    "    text2 = url_response.messages[-1]['content']\n",
    "    print(text2)\n",
    "    # Regex pattern to find URLs\n",
    "    url_pattern = r'\\((https?://[^\\s)]+)\\)'\n",
    "\n",
    "    # Find all unique URLs in the text\n",
    "    urls = set(re.findall(url_pattern, text2))\n",
    "\n",
    "    # Print the unique URLs\n",
    "    print(f\"urls :{urls}\")\n",
    "    print(list(urls))\n",
    "    #extract content form the url\n",
    "    content = extract_content(list(urls)[0])\n",
    "    print(f\"Content :{content}\")\n",
    "    #Step 2 Generate Summary\n",
    "    summary_response = client.run(\n",
    "        agent=content_agent,\n",
    "        messages=[{\"role\": \"user\", \"content\": content }],\n",
    "    )\n",
    "    print(summary_response.messages[-1][\"content\"])\n",
    "    return summary_response.messages[-1][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkRag\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6409/2044149504.py:20: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  for result in search.results():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkRAG\n",
      "Based on the information provided:\n",
      "\n",
      "- **Title**: ChunkRAG: Novel LLM-Chunk Filtering Method for RAG Systems\n",
      "- **Date**: 2024-10-25\n",
      "- **URL**: [http://arxiv.org/pdf/2410.19572v4](http://arxiv.org/pdf/2410.19572v4)\n",
      "- **Summary**:\n",
      "  Retrieval-Augmented Generation (RAG) systems using large language models\n",
      "  (LLMs) often generate inaccurate responses due to the retrieval of irrelevant or loosely related information.\n",
      "  Existing methods, which operate at the document level, fail to effectively filter out such content.\n",
      "  \n",
      "  We propose LLM-driven chunk filtering, ChunkRAG, a framework that enhances RAG systems by evaluating and filtering retrieved\n",
      "  information at the chunk level. Our approach employs semantic chunking to divide documents into coherent sections and utilizes\n",
      "  LLM-based relevance scoring to assess each chunk's alignment with the user’s query.\n",
      "  \n",
      "  By filtering out less pertinent chunks before the generation phase, we significantly reduce hallucinations and improve factual accuracy. Experiments show that our method outperforms existing RAG models,\n",
      "  achieving higher accuracy on tasks requiring precise information retrieval.\n",
      "  \n",
      "  This advancement enhances the reliability of RAG systems, making them particularly beneficial for applications such as fact-checking\n",
      "and multi-hop reasoning.\n",
      "\n",
      "If you need more details or further analysis, please let me know!\n",
      "The URL provided in the content is:\n",
      "\n",
      "[http://arxiv.org/pdf/2410.19572v4](http://arxiv.org/pdf/2410.19572v4)\n",
      "urls :{'http://arxiv.org/pdf/2410.19572v4'}\n",
      "['http://arxiv.org/pdf/2410.19572v4']\n",
      "Content :ChunkRAG: Novel LLM-Chunk Filtering Method for RAG Systems\n",
      "\n",
      "Ishneet Sukhvinder Singh*\n",
      "\n",
      "Ritvik Aggarwal*\n",
      "\n",
      "Aslihan Akalin\n",
      "\n",
      "Kevin Zhu\n",
      "\n",
      "Ibrahim Allahverdiyev\n",
      "Sean O’Brien\n",
      "\n",
      "Muhammad Taha\n",
      "\n",
      "Algoverse AI Research\n",
      "asli@algoverse.us, kevin@algoverse.us, seobrien@ucsd.edu\n",
      "\n",
      "4\n",
      "2\n",
      "0\n",
      "2\n",
      "\n",
      "v\n",
      "o\n",
      "N\n",
      "9\n",
      "1\n",
      "\n",
      "]\n",
      "L\n",
      "C\n",
      ".\n",
      "s\n",
      "c\n",
      "[\n",
      "\n",
      "4\n",
      "v\n",
      "2\n",
      "7\n",
      "5\n",
      "9\n",
      "1\n",
      ".\n",
      "0\n",
      "1\n",
      "4\n",
      "2\n",
      ":\n",
      "v\n",
      "i\n",
      "X\n",
      "r\n",
      "a\n",
      "\n",
      "Abstract\n",
      "\n",
      "Retrieval-Augmented Generation (RAG) sys-\n",
      "tems using large language models (LLMs) of-\n",
      "ten generate inaccurate responses due to the\n",
      "retrieval of irrelevant or loosely related infor-\n",
      "mation. Existing methods, which operate at the\n",
      "document level, fail to effectively filter out such\n",
      "content. We propose LLM-driven chunk filter-\n",
      "ing, ChunkRAG, a framework that enhances\n",
      "RAG systems by evaluating and filtering re-\n",
      "trieved information at the chunk level, where\n",
      "a \"chunk\" represents a smaller, coherent sec-\n",
      "tion of a document. Our approach employs\n",
      "semantic chunking to divide documents into\n",
      "coherent sections and utilizes LLM-based rel-\n",
      "evance scoring to assess each chunk’s align-\n",
      "ment with the user’s query. By filtering out less\n",
      "pertinent chunks before the generation phase,\n",
      "we significantly reduce hallucinations and im-\n",
      "prove factual accuracy. Experiments show that\n",
      "our method outperforms existing RAG models,\n",
      "achieving higher accuracy on tasks requiring\n",
      "precise information retrieval. This advance-\n",
      "ment enhances the reliability of RAG systems,\n",
      "making them particularly beneficial for applica-\n",
      "tions like fact-checking and multi-hop reason-\n",
      "ing.\n",
      "\n",
      "1\n",
      "\n",
      "Introduction\n",
      "\n",
      "Large language models (LLMs) have made sig-\n",
      "nificant strides in the development of retrieval-\n",
      "augmented generation (RAG) systems, which com-\n",
      "bine retrieval mechanisms with powerful language\n",
      "models to produce responses based on external\n",
      "knowledge. However, despite these advancements,\n",
      "a persistent issue remains: the retrieval of irrelevant\n",
      "or weakly related information during the document-\n",
      "fetching process. Current retrieval techniques, in-\n",
      "cluding reranking and query rewriting, not only fail\n",
      "to filter out lots of irrelevant chunks of informa-\n",
      "tion in the retrieved documents but also lead to a\n",
      "series of problems with factual inaccuracies, irrel-\n",
      "evance, and hallucinations in the responses gener-\n",
      "\n",
      "1\n",
      "\n",
      "Figure 1: Comparison of Response Generation With\n",
      "and Without Chunk Filtering\n",
      "\n",
      "ated (Zhang and Others, 2023; Mallen et al., 2023).\n",
      "\n",
      "Traditionally, RAG systems retrieve large\n",
      "amounts of the text of entire documents or lengthy\n",
      "portions thereof, assuming that it is likely that these\n",
      "lengthy fragments will contain the relevant informa-\n",
      "tion. Such systems very rarely examine the sections\n",
      "or paragraphs of the retrieved documents individ-\n",
      "ually and, therefore, there is a strong likelihood\n",
      "that irrelevant or only partially related information\n",
      "will flow into the generation stage. This is fur-\n",
      "ther worsened by the fact that language models\n",
      "generate fluent text without being able to verify\n",
      "the information they use for generation. Relevant\n",
      "or misleading chunks distort the outcome of such\n",
      "models severely, reducing the system’s reliability,\n",
      "especially in critical tasks such as open-domain\n",
      "question answering and multi-hop reasoning (Ji\n",
      "et al., 2023; Min et al., 2023).\n",
      "\n",
      "Fig. 1: The figure shows that without chunk\n",
      "filtering (top), irrelevant information like other\n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\fFrench cities is included in the response. The\n",
      "LLM-driven chunk filtering (bottom), however, re-\n",
      "moves unnecessary content, delivering the precise\n",
      "answer, \"The capital of France is Paris.\" A few\n",
      "retrieval-related methods, Corrective RAG (CRAG)\n",
      "and Self-RAG, have attempted to overcome these\n",
      "hurdles by refining the retrieval process. CRAG fo-\n",
      "cuses on retrieving \"corrections\" after errors occur\n",
      "in retrieval, whereas Self-RAG introduces a self-\n",
      "reflection mechanism during the generation stage\n",
      "to minimize inaccuracies. Both of these processes\n",
      "operate at the document level and lack sufficient fil-\n",
      "tering for individual retrieved chunks of text. This\n",
      "document-level approach enhances the overall rele-\n",
      "vance of the retrieval but does not prevent irrelevant\n",
      "chunks from being included in the generated re-\n",
      "sponse (Shi et al., 2023). Without control over the\n",
      "granularity of the retrieved content, RAG systems\n",
      "remain vulnerable to incorporating undesirable or\n",
      "misleading information into their output, ultimately\n",
      "compromising performance.\n",
      "\n",
      "The solution to this challenge lies in the novel ap-\n",
      "proach: LLM-driven chunk filtering, ChunkRAG.\n",
      "Our method operates on a finer level of granularity\n",
      "than classical systems and, in fact, supports chunk-\n",
      "level filtering of retrieved information. Rather than\n",
      "judging entire documents to be relevant, our system\n",
      "goes both for the user query and individual chunks\n",
      "within retrieved documents. The large language\n",
      "model evaluates semantic relevance of each chunk\n",
      "with respect to the user’s query; this makes the sys-\n",
      "tem capable of filtering out irrelevant or weakly\n",
      "related chunks even before they get into the gener-\n",
      "ation stage. This chunk-level filtering in turn aims\n",
      "to enforce factual accuracy on the final answer by\n",
      "drawing only the most relevant information on the\n",
      "generation. This approach is particularly promising\n",
      "for knowledge-intensive tasks, such as multi-hop\n",
      "reasoning and fact-checking: precision is the ulti-\n",
      "mate prize here. That is, in tasks where accuracy is\n",
      "paramount, our approach stands best (Piktus et al.,\n",
      "2021; Rony et al., 2022).\n",
      "\n",
      "2 Related Works\n",
      "\n",
      "Addressing Hallucinations in Large Language\n",
      "Models Large language models (LLMs) have made\n",
      "significant strides in understanding instructions and\n",
      "generating coherent text (Bang et al., 2023; Qin et\n",
      "al., 2023; Zhong et al., 2023). However, they still\n",
      "grapple with the issue of hallucinations, where the\n",
      "\n",
      "models produce outputs that are incorrect or non-\n",
      "sensical. Research indicates that the activation of\n",
      "outdated or erroneous knowledge contributes to this\n",
      "problem (Tonmoy et al., 2024; Zhang et al., 2023b;\n",
      "Shuster et al., 2021). Factors such as reliance on\n",
      "large, unregulated datasets, a low proportion of\n",
      "high-quality training samples, and suboptimal data\n",
      "distribution within the input space exacerbate these\n",
      "challenges. The absence of precise and accurate\n",
      "knowledge often leads to misleading or inaccurate\n",
      "outputs, severely impacting user experience in prac-\n",
      "tical applications.\n",
      "\n",
      "Retrieval-Augmented Generation Retrieval-\n",
      "Augmented Generation (RAG) has been proposed\n",
      "as an effective strategy to mitigate hallucinations\n",
      "(Lewis et al., 2020; Guu et al., 2020). By augment-\n",
      "ing input queries with retrieved documents from\n",
      "specific corpora like Wikipedia, RAG provides ad-\n",
      "ditional knowledge that enhances the performance\n",
      "of LLMs, especially in tasks that are knowledge-\n",
      "intensive. This approach involves using informa-\n",
      "tion retrieval systems to supply relevant documents\n",
      "to the generative models. Early implementations\n",
      "employed either sparse or dense retrievers preced-\n",
      "ing a pretrained language model focused on re-\n",
      "sponse generation. However, these methods often\n",
      "overlook a critical question: What happens if the re-\n",
      "trieval process fails or retrieves irrelevant informa-\n",
      "tion? Irrelevant documents can worsen the factual\n",
      "inaccuracies of the model’s output, counteracting\n",
      "the benefits of retrieval augmentation.\n",
      "\n",
      "Advancements in Retrieval Techniques Re-\n",
      "cent developments have aimed to refine RAG meth-\n",
      "ods to address these limitations (Zhang et al.,\n",
      "2024; Kim et al., 2024; Wang et al., 2024; Liu\n",
      "et al., 2024). Recognizing that retrieval is not al-\n",
      "ways necessary—and can sometimes decrease accu-\n",
      "racy—approaches like SelfRAG (Asai et al., 2024)\n",
      "incorporate mechanisms to selectively decide when\n",
      "to retrieve information, using a critic model for\n",
      "this purpose. CRAG (Your et al., 2024) is a re-\n",
      "cent approach that augments standard RAG with\n",
      "corrective strategies to improve retrieval quality by\n",
      "addressing low-quality retrieval results. Yoran et\n",
      "al. (2024) introduced a Natural Language Infer-\n",
      "ence (NLI) model to detect and filter out irrelevant\n",
      "contexts, enhancing the robustness of the system.\n",
      "SAIL (Luo et al., 2023) fine-tunes models to insert\n",
      "retrieved documents before processing instructions,\n",
      "improving the integration of external knowledge.\n",
      "Toolformer (Schick et al., 2023) pretrains models\n",
      "to interact with APIs like Wikipedia, enabling dy-\n",
      "\n",
      "2\n",
      "\n",
      "\fnamic access to information. In scenarios involving\n",
      "long-form text generation, where external knowl-\n",
      "edge may be needed multiple times, determining\n",
      "the optimal timing for retrieval becomes crucial.\n",
      "Jiang et al. (2023) propose anticipating future con-\n",
      "tent needs to decide when and what information to\n",
      "retrieve during the generation process.\n",
      "\n",
      "Redundancy in retrieved information can dimin-\n",
      "ish the effectiveness of RAG models by introduc-\n",
      "ing repetitive or irrelevant data, which hampers the\n",
      "model’s ability to generate coherent and unique\n",
      "responses. One prevalent approach to mitigating\n",
      "redundancy involves the use of cosine similarity\n",
      "to evaluate and remove duplicate or overly similar\n",
      "content from the retrieved documents.\n",
      "\n",
      "Cosine Similarity in Redundancy Removal\n",
      "Cosine similarity measures the cosine of the angle\n",
      "between two non-zero vectors of an inner product\n",
      "space, which quantifies the similarity between the\n",
      "In\n",
      "two vectors irrespective of their magnitude.\n",
      "the context of RAG, it is employed to compare\n",
      "textual embeddings of retrieved chunks to identify\n",
      "and eliminate redundant content, enhancing the\n",
      "diversity of the information available for generation\n",
      "(Liu et al., 2023).\n",
      "\n",
      "Multi-Meta-RAG for Multi-Hop Queries Ad-\n",
      "dressing the challenges of multi-hop queries, Multi-\n",
      "Meta-RAG introduces a database filtering mecha-\n",
      "nism using metadata extracted by large language\n",
      "models (LLMs). By incorporating LLM-extracted\n",
      "metadata, this approach filters databases to retrieve\n",
      "more relevant documents that contribute to answer-\n",
      "ing complex queries requiring reasoning over multi-\n",
      "ple pieces of information (Smith et al., 2023). This\n",
      "method reduces redundancy by ensuring that only\n",
      "pertinent documents are considered, thereby im-\n",
      "proving the coherence of the generated responses.\n",
      "Query Rewriting for Enhanced Retrieval A\n",
      "\"Rewrite-Retrieve-Read\" framework to bridge the\n",
      "gap between input text and the necessary retrieval\n",
      "knowledge is proposed (Johnson and Lee, 2023).\n",
      "A trainable query rewriter adapts queries using re-\n",
      "inforcement learning based on feedback from the\n",
      "LLM’s performance. This approach enhances re-\n",
      "trieval accuracy by reformulating queries to better\n",
      "align with relevant documents, thus minimizing the\n",
      "retrieval of redundant or irrelevant information.\n",
      "\n",
      "We introduce a new model, ChunkRAG, that\n",
      "emphasizes a chunking strategy aimed at further\n",
      "reducing redundancy and improving the effective-\n",
      "ness of RAG models. Compared with recent studies\n",
      "(Schick et al., 2023; Luo et al., 2023; Asai et al.,\n",
      "\n",
      "2024, Your et al., 2024), our approach involves seg-\n",
      "menting documents into semantically coherent and\n",
      "non-overlapping chunks that are more aligned with\n",
      "the specific information needs of the query.\n",
      "\n",
      "3 Methodology\n",
      "\n",
      "The core objective of this work is to mitigate irrele-\n",
      "vance and hallucinations in the responses generated\n",
      "by Retrieval-Augmented Generation (RAG) sys-\n",
      "tems, using a novel, fine-grained filtering mecha-\n",
      "nism that rigorously evaluates the relevance of each\n",
      "chunk of retrieved information before integrating\n",
      "it into the response generation phase. Our pro-\n",
      "posed methodology follows a two-stage approach:\n",
      "semantic chunking and advanced filtering to refine\n",
      "retrieval results. Each stage is designed to enhance\n",
      "the system’s precision and reliability in leveraging\n",
      "retrieved knowledge. Below, we detail the compo-\n",
      "nents of our proposed method.\n",
      "\n",
      "Figure 2: ChunkRAG Methodology for Enhanced\n",
      "Retrieval and Filtering\n",
      "\n",
      "Semantic Chunking\n",
      "\n",
      "Semantic chunking serves as the foundational step\n",
      "of our methodology, transforming the input docu-\n",
      "ment into semantically meaningful units to facili-\n",
      "tate effective retrieval and evaluation. This stage\n",
      "involves three sub-processes:\n",
      "\n",
      "• Input Preparation: We begin by tokenizing\n",
      "a document D into sentences using NLTK’s\n",
      "\n",
      "3\n",
      "\n",
      "\fsent_tokenize function. Each sentence is\n",
      "then assigned an embedding vector, generated\n",
      "using a pre-trained embedding model (e.g.,\n",
      "text-embedding-3-small).\n",
      "\n",
      "• Chunk Formation: Consecutive sentences are\n",
      "grouped into chunks based on their seman-\n",
      "tic similarity, measured through cosine simi-\n",
      "larity. Specifically, if the similarity between\n",
      "consecutive sentences drops below a thresh-\n",
      "old (θ = 0.7), a new chunk is created. Each\n",
      "chunk is further constrained to be under 500\n",
      "characters to ensure efficiency during subse-\n",
      "quent stages.\n",
      "\n",
      "• Embedding Generation for Chunks:\n",
      "\n",
      "Each\n",
      "chunk is represented using the same pre-\n",
      "trained embedding model as above. The\n",
      "resultant chunk embeddings are stored in a\n",
      "vector database to facilitate efficient retrieval\n",
      "during the query phase.\n",
      "\n",
      "Hybrid Retrieval and Advanced Filtering\n",
      "\n",
      "In the retrieval and filtering phase, we integrate\n",
      "conventional RAG components with advanced fine-\n",
      "tuning techniques to ensure robust and high-quality\n",
      "retrieval. The hybrid retrieval and filtering stage is\n",
      "detailed below:\n",
      "\n",
      "• Retriever Initialization and Query Rewriting:\n",
      "We initialize a retriever capable of comparing\n",
      "user queries against the chunk embeddings.\n",
      "To enhance query efficacy, we apply a query\n",
      "rewriting step using GPT-4omini, ensuring\n",
      "that the queries are well-matched to the stored\n",
      "embeddings. This ensures better recall and\n",
      "precision in the retrieval process.\n",
      "\n",
      "• Initial Filtering: Retrieved chunks are initially\n",
      "filtered using a combination of TF-IDF scor-\n",
      "ing and cosine similarity. Chunks with high\n",
      "redundancy (similarity > 0.9) are eliminated.\n",
      "The remaining chunks are sorted based on\n",
      "their similarity to the rewritten query.\n",
      "\n",
      "• Relevance Scoring and Thresholding: To fur-\n",
      "ther refine relevance, each chunk is assigned\n",
      "an initial relevance score by a large lan-\n",
      "guage model (LLM). These scores are refined\n",
      "through self-reflection and a critic model,\n",
      "which adjusts the scores based on domain-\n",
      "specific heuristics. A final dynamic threshold\n",
      "is set by analyzing the score distribution, and\n",
      "\n",
      "only chunks surpassing this threshold are re-\n",
      "tained.\n",
      "\n",
      "• Hybrid Retrieval Strategy: To maximize re-\n",
      "trieval effectiveness, we employ a dual\n",
      "retrieval strategy combining BM25 and\n",
      "LLM-based retrieval methods.\n",
      "The en-\n",
      "semble approach uses equal weighting (0.5\n",
      "each) to balance keyword and semantic re-\n",
      "trieval.\n",
      "Furthermore, Cohere’s reranking\n",
      "model (rerank-englishv3.0) is used to rank\n",
      "the retrieved chunks, addressing the Lost in\n",
      "the middle problem by enhancing the rele-\n",
      "vance of central context that might otherwise\n",
      "be deprioritized.\n",
      "\n",
      "Response Generation and Evaluation\n",
      "\n",
      "After filtering, the remaining chunks are used as\n",
      "context to generate the final response. The steps\n",
      "include:\n",
      "\n",
      "• Response Generation: An LLM generates\n",
      "a response based on the filtered context\n",
      "chunks. During generation, strict constraints\n",
      "ensure that only retrieved information is used,\n",
      "thereby minimizing the risk of hallucinations.\n",
      "\n",
      "• Evaluation: The generated responses are\n",
      "evaluated for accuracy against a set of pre-\n",
      "validated answers.\n",
      "\n",
      "Our methodology, combining semantic chunking\n",
      "with advanced retrieval and filtering mechanisms,\n",
      "significantly enhances the quality of responses pro-\n",
      "duced by RAG systems, ensuring both relevance\n",
      "and correctness of the generated content. The em-\n",
      "pirical results, as described in subsequent sections,\n",
      "demonstrate the effectiveness of our approach in\n",
      "various retrieval and generation scenarios.\n",
      "\n",
      "4 Experiments\n",
      "\n",
      "We conducted experiments to extensively demon-\n",
      "strate ChunkRAG’s adaptability and its poten-\n",
      "tial for generalizability across various generation\n",
      "tasks. However, due to computational resource con-\n",
      "straints, our evaluation was primarily focused on\n",
      "the PopQA dataset.\n",
      "\n",
      "4.1 Tasks and Datasets\n",
      "\n",
      "ChunkRAG was evaluated on the PopQA dataset,\n",
      "which serves as the cornerstone of our experimen-\n",
      "tal analysis. PopQA (Mallen et al., 2023) is a\n",
      "\n",
      "4\n",
      "\n",
      "\fbenchmark dataset designed for short-form ques-\n",
      "tion answering. It comprises a diverse set of ques-\n",
      "tions that require concise and accurate responses,\n",
      "making it an ideal testbed for assessing the perfor-\n",
      "mance of retrieval-augmented generation models\n",
      "like ChunkRAG.\n",
      "\n",
      "To measure the effectiveness of ChunkRAG, we\n",
      "adopted accuracy as the evaluation metric, consis-\n",
      "tent with prior studies. This metric aligns with the\n",
      "conventions used in the evaluation of PopQA, en-\n",
      "suring that our results are comparable to existing\n",
      "research.\n",
      "\n",
      "While our current experiments are limited to\n",
      "PopQA, ChunkRAG is architected with scalabil-\n",
      "ity in mind. Future evaluations may extend to ad-\n",
      "ditional datasets such as Biography (Min et al.,\n",
      "2023) for long-form generation, PubHealth (Zhang\n",
      "et al., 2023) for true/false question answering, and\n",
      "Arc-Challenge (Bhakthavatsalam et al., 2021) for\n",
      "multiple-choice questions. These extensions will\n",
      "further validate ChunkRAG’s versatility across dif-\n",
      "ferent types of generation tasks, contingent upon\n",
      "the availability of computational resources.\n",
      "\n",
      "4.2 Baselines\n",
      "\n",
      "4.2.1 Baselines Without Retrieval\n",
      "We first evaluated several large language models\n",
      "(LLMs) that do not incorporate retrieval mecha-\n",
      "nisms. Among the public LLMs, we included\n",
      "LLaMA2-7B and LLaMA2-13B (Touvron et al.,\n",
      "2023), known for their versatility across diverse nat-\n",
      "ural language processing (NLP) tasks, and Alpaca-\n",
      "7B and Alpaca-13B (Dubois et al., 2023), which are\n",
      "instruction-tuned models optimized for effectively\n",
      "following user prompts. Additionally, we assessed\n",
      "CoVE65B (Dhuliawala et al., 2024), which intro-\n",
      "duces iterative engineering techniques aimed at en-\n",
      "hancing the factual accuracy of generated content.\n",
      "For proprietary models, we included LLaMA2-\n",
      "chat13B, a conversational variant of LLaMA2 tai-\n",
      "lored for dialogue-based applications, and Chat-\n",
      "GPT, OpenAI’s proprietary conversational agent\n",
      "renowned for its robust language understanding\n",
      "and generation capabilities.\n",
      "\n",
      "4.2.2 Baselines With Retrieval\n",
      "Standard Retrieval-Augmented Generation\n",
      "(RAG): To establish a baseline for retrieval-\n",
      "augmented methods, we evaluated standard RAG\n",
      "approaches. Specifically, we employed Standard\n",
      "RAG (Lewis et al., 2020), which utilizes a retriever\n",
      "to fetch relevant documents based on the input\n",
      "\n",
      "query, subsequently feeding these documents into\n",
      "the language model to generate responses. For\n",
      "consistency, we utilized the same retriever mech-\n",
      "anism as ChunkRAG to ensure a fair compari-\n",
      "son. In addition to Standard RAG, we evaluated\n",
      "instruction-tuned LLMs with standard RAG, in-\n",
      "cluding LLaMA2-7B, LLaMA2-13B, and Alpaca-\n",
      "7B, Alpaca-13B, to assess the impact of instruction\n",
      "tuning in conjunction with retrieval augmentation.\n",
      "Advanced Retrieval-Augmented Generation:\n",
      "To benchmark ChunkRAG against more sophisti-\n",
      "cated RAG-based methods, we included advanced\n",
      "models that incorporate additional strategies to en-\n",
      "hance performance. SAIL (Luo et al., 2023) en-\n",
      "hances standard RAG by instruction-tuning the lan-\n",
      "guage model on Alpaca instruction-tuning data, in-\n",
      "serting top retrieved documents before the instruc-\n",
      "tions to provide contextual information. Self-RAG\n",
      "(Asai et al., 2024) further refines RAG by incorpo-\n",
      "rating reflection tokens labeled by GPT-4 within\n",
      "the instruction-tuning data, enabling the model to\n",
      "better utilize retrieved information. Additionally,\n",
      "we considered CRAG (Your et al., 2024), a recent\n",
      "approach that augments standard RAG with correc-\n",
      "tive strategies to improve retrieval quality by ad-\n",
      "dressing low-quality retrieval results. CRAG serves\n",
      "as a direct comparison to our proposed ChunkRAG,\n",
      "highlighting the effectiveness of our chunk filter-\n",
      "ing mechanism in enhancing retrieval-augmented\n",
      "generation. Furthermore, we evaluated retrieval-\n",
      "augmented baselines with private data, including\n",
      "Ret-ChatGPT and RetLLaMA-chat, which inte-\n",
      "grate retrieval mechanisms with ChatGPT and the\n",
      "conversational variant of LLaMA2, respectively.\n",
      "\n",
      "5 Analysis\n",
      "\n",
      "In this section, we evaluate the performance of\n",
      "ChunkRAG against existing retrieval-augmented\n",
      "generation (RAG) methods. We present an analysis\n",
      "based on empirical results obtained from standard\n",
      "benchmarks.\n",
      "\n",
      "5.1 Evaluation Metrics\n",
      "\n",
      "We used accuracy as the primary evaluation metric,\n",
      "calculated as the percentage of generated responses\n",
      "that match the ground-truth answers.\n",
      "\n",
      "5.2 Comparison and Impact\n",
      "\n",
      "As depicted in Table 1, our method achieved an\n",
      "accuracy of 64.9, substantially outperforming all\n",
      "baselines in the same category. Notably, compared\n",
      "\n",
      "5\n",
      "\n",
      "\fTable 1: Performance Comparison Across Methods\n",
      "(PopQA Accuracy Only)\n",
      "\n",
      "Method\n",
      "\n",
      "PopQA (Accuracy)\n",
      "\n",
      "LLMs trained with proprietary data\n",
      "\n",
      "LLaMA2-C_13B\n",
      "Ret-LLaMA2-C_13B\n",
      "ChatGPT\n",
      "Ret-ChatGPT\n",
      "\n",
      "20.0\n",
      "51.8\n",
      "29.3\n",
      "50.8\n",
      "Baselines without retrieval\n",
      "14.7\n",
      "23.6\n",
      "14.7\n",
      "14.3\n",
      "-\n",
      "Baselines with retrieval\n",
      "\n",
      "LLaMA2_7B\n",
      "Alpaca_7B\n",
      "LLaMA2_13B\n",
      "Alpaca_13B\n",
      "CoVE_65B\n",
      "\n",
      "LLaMA2_7B\n",
      "Alpaca_7B\n",
      "SAIL\n",
      "LLaMA2_13B\n",
      "Alpaca_13B\n",
      "\n",
      "LLaMA2-hf_7B\n",
      "\n",
      "RAG\n",
      "CRAG\n",
      "Self-RAG\n",
      "Self-CRAG\n",
      "ChunkRAG\n",
      "\n",
      "38.2\n",
      "46.7\n",
      "-\n",
      "45.7\n",
      "46.1\n",
      "\n",
      "50.5\n",
      "54.9\n",
      "50.5\n",
      "49.0\n",
      "64.9\n",
      "\n",
      "to the closest baseline, CRAG (54.9 accuracy), our\n",
      "method exhibits a performance gain of 10 percent-\n",
      "age points.\n",
      "\n",
      "While a 10 percentage point increase may seem\n",
      "incremental, it translates into an exponential im-\n",
      "provement in output effectiveness in practical ap-\n",
      "plications. This is particularly evident when consid-\n",
      "ering the error rates and their impact on the overall\n",
      "user experience.In applications requiring multi-hop\n",
      "reasoning or sequential decision-making, errors\n",
      "can compound exponentially. This exponential\n",
      "improvement is especially important in complex\n",
      "tasks where each additional step compounds the\n",
      "risk of error, namely relevant to OpenAI’s advanced\n",
      "models such as o1 where the language model uti-\n",
      "lizes multi-hop reasoning, relying on spending time\n",
      "\"thinking\" before it answers, making it more effi-\n",
      "cient in complex reasoning tasks, science and pro-\n",
      "gramming.\n",
      "\n",
      "5.3 Observations and Insights\n",
      "\n",
      "The notable improvement attained with our tech-\n",
      "nique is mainly due to chunk-level filtering and\n",
      "fine-grained relevance assessment. We divided\n",
      "the text into semantically meaningful chunks,\n",
      "which reduced the generation of irrelevant or\n",
      "In processing the\n",
      "weakly related information.\n",
      "chunk filtering’s contextually relevant data, the gen-\n",
      "eration of factually accurate and coherent responses\n",
      "was significantly enhanced.\n",
      "\n",
      "6\n",
      "\n",
      "Moreover,\n",
      "\n",
      "the self-reflective LLM scoring\n",
      "method, in which the model grades itself and then\n",
      "changes accordingly, led to a significant decrease\n",
      "in retrieval errors. Unlike regular retrieval methods\n",
      "that do not have a filtering mechanism at the doc-\n",
      "ument section level, our method can extract more\n",
      "meaningful and relevant information that directly\n",
      "affects the reliability of the generated responses.\n",
      "\n",
      "5.4 Future Work\n",
      "\n",
      "In our present studies, we have only tested PopQA\n",
      "but the design of ChunkRAG is for scalability\n",
      "purposes. In the upcoming assessments, we will\n",
      "also introduce new datasets including Biography\n",
      "for long-form generation, PubHealth for true/false\n",
      "questions, and Arc-Challenge for multiple-choice\n",
      "questions. The implementation of these trials will\n",
      "thus reinforce the evidence of ChunkRAG’s versa-\n",
      "tility and adaptability to different types of genera-\n",
      "tion tasks, although this will be conditional on the\n",
      "availability of computing resources.\n",
      "\n",
      "6 Conclusion\n",
      "\n",
      "In this paper, we introduced ChunkRAG, a novel\n",
      "LLM-driven chunk filtering approach aimed at im-\n",
      "proving the precision and factuality of retrieval-\n",
      "augmented generation systems. In our experiments,\n",
      "which were conducted on the PopQA dataset,\n",
      "ChunkRAG has clearly demonstrated superiority\n",
      "over existing baselines, and thus has achieved a\n",
      "significant performance boost of 10 percentage\n",
      "points, which was higher than the closest bench-\n",
      "mark, CRAG. The chunk-level filtering technique\n",
      "guaranteed that only the relevant and contextually\n",
      "correct information was included during the re-\n",
      "sponse generation, resulting in better reliability\n",
      "and accuracy of generated answers. This method is\n",
      "particularly useful for applications that require im-\n",
      "mense amounts of facts, such as multi-hop reason-\n",
      "ing and decision-making that involve many interde-\n",
      "pendent parameters. We believe that ChunkRAG\n",
      "is a big step towards solving the problems of ir-\n",
      "relevant or hallucinated material in LLM-based\n",
      "retrieval systems.\n",
      "\n",
      "7 Limitations\n",
      "\n",
      "ChunkRAG, in spite of its benefits, has a number\n",
      "of drawbacks that need to be taken into account.\n",
      "Firstly, the method relies heavily on the effective-\n",
      "ness of chunk segmentation and the quality of the\n",
      "embeddings used for chunk relevance assessment.\n",
      "\n",
      "\fMistakes in the primary division can create irrel-\n",
      "evant data that will decrease the quality of the re-\n",
      "sponse. Secondly, the costs from the multi-level\n",
      "score—integrating both LLM and critic LLM evalu-\n",
      "ations at the initial level—can be high, particularly\n",
      "during the scaling of the method to larger datasets\n",
      "or the deployment of it in real-time systems. Addi-\n",
      "tionally, while ChunkRAG demonstrated positive\n",
      "outcomes in the use of the PopQA dataset, the\n",
      "verifiability of its use in other domains and the per-\n",
      "formance when operating through long-form gen-\n",
      "eration tasks has not been thoroughly analyzed due\n",
      "to resource limitations. Future studies should con-\n",
      "centrate on the optimization of the computational\n",
      "efficiency of ChunkRAG and its evaluation over\n",
      "diverse datasets and in real-world applications.\n",
      "\n",
      "References\n",
      "\n",
      "A. Asai et al. 2024. Self-rag: Self-reflective retrieval-\n",
      "augmented generation for knowledge-intensive tasks.\n",
      "In Proceedings of the Annual Meeting of the Associa-\n",
      "tion for Computational Linguistics (ACL).\n",
      "\n",
      "S. Min et al. 2023. Self-reflective mechanisms for im-\n",
      "proved retrieval-augmented generation. In Proceed-\n",
      "ings of the 61st Annual Meeting of the Association\n",
      "for Computational Linguistics (ACL).\n",
      "\n",
      "A. Piktus et al. 2021. The role of chunking in retrieval-\n",
      "augmented generation. In Proceedings of the Con-\n",
      "ference on Neural Information Processing Systems\n",
      "(NeurIPS).\n",
      "\n",
      "M. S. Rony et al. 2022. Fine-grained document retrieval\n",
      "for fact-checking tasks. In Proceedings of the 2022\n",
      "Conference on Empirical Methods in Natural Lan-\n",
      "guage Processing (EMNLP).\n",
      "\n",
      "Y. Shi et al. 2023. Corrective retrieval in retrieval-\n",
      "augmented generation systems. In Proceedings of\n",
      "the International Conference on Machine Learning\n",
      "(ICML).\n",
      "\n",
      "T. Smith et al. 2023. Multi-meta-rag for multi-hop\n",
      "queries using llm-extracted metadata. In Proceedings\n",
      "of the International Conference on Computational\n",
      "Linguistics (COLING).\n",
      "\n",
      "H. Touvron et al. 2023.\n",
      "\n",
      "large language models.\n",
      "\n",
      "ficient\n",
      "arXiv:2307.12345.\n",
      "\n",
      "Llama2: Open and ef-\n",
      "arXiv preprint\n",
      "\n",
      "S. Bhakthavatsalam et al. 2021. Multi-hop reasoning\n",
      "In Proceedings of the\n",
      "with graph-based retrieval.\n",
      "59th Annual Meeting of the Association for Compu-\n",
      "tational Linguistics (ACL).\n",
      "\n",
      "S. Your et al. 2024. Crag: Corrective retrieval-\n",
      "In Proceedings of the An-\n",
      "augmented generation.\n",
      "nual Meeting of the Association for Computational\n",
      "Linguistics (ACL).\n",
      "\n",
      "A. Zhang and Others. 2023. Another title of the paper.\n",
      "\n",
      "arXiv preprint arXiv:2302.56789.\n",
      "\n",
      "A. Zhang et al. 2023. Hallucination in large language\n",
      "models: A comprehensive survey. arXiv preprint\n",
      "arXiv:2301.12345.\n",
      "\n",
      "F. Dhuliawala et al. 2024. Cove65b: Enhancing fac-\n",
      "tual accuracy through iterative engineering. arXiv\n",
      "preprint arXiv:2401.12345.\n",
      "\n",
      "Y. Dubois et al. 2023.\n",
      "\n",
      "Instruction tuning for open-\n",
      "domain question answering. In Advances in Neural\n",
      "Information Processing Systems (NeurIPS).\n",
      "\n",
      "Z. Ji et al. 2023. Survey of hallucination in generative\n",
      "\n",
      "models. arXiv preprint arXiv:2302.02451.\n",
      "\n",
      "R. Johnson and T. Lee. 2023. Query rewriting for\n",
      "retrieval-augmented large language models. In Pro-\n",
      "ceedings of the International Conference on Machine\n",
      "Learning (ICML).\n",
      "\n",
      "P. Lewis et al. 2020. Retrieval-augmented generation\n",
      "for knowledge-intensive nlp tasks. In Advances in\n",
      "Neural Information Processing Systems, volume 33,\n",
      "pages 9459–9474.\n",
      "\n",
      "S. Liu et al. 2023. Redundancy removal in retrieval-\n",
      "augmented generation using cosine similarity.\n",
      "In\n",
      "Proceedings of the Conference on Empirical Methods\n",
      "in Natural Language Processing (EMNLP).\n",
      "\n",
      "H. Luo et al. 2023. Sail: Instruction tuning for enhanced\n",
      "\n",
      "retrieval-augmented generation.\n",
      "\n",
      "J. Mallen et al. 2023. Enhancing retrieval-augmented\n",
      "In Proceedings of\n",
      "generation with fact-checking.\n",
      "the Conference on Empirical Methods in Natural\n",
      "Language Processing (EMNLP).\n",
      "\n",
      "7\n",
      "\n",
      "\f\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m topic \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChunkRag\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mrun_arxiv_paper_summary_workflow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtopic\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[26], line 27\u001b[0m, in \u001b[0;36mrun_arxiv_paper_summary_workflow\u001b[0;34m(topic)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent :\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m#Step 2 Generate Summary\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m summary_response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontent_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(summary_response\u001b[38;5;241m.\u001b[39mmessages[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m summary_response\u001b[38;5;241m.\u001b[39mmessages[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/semantic_research_engine/lib/python3.10/site-packages/swarm/core.py:260\u001b[0m, in \u001b[0;36mSwarm.run\u001b[0;34m(self, agent, messages, context_variables, model_override, stream, debug, max_turns, execute_tools)\u001b[0m\n\u001b[1;32m    255\u001b[0m init_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(messages)\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(history) \u001b[38;5;241m-\u001b[39m init_len \u001b[38;5;241m<\u001b[39m max_turns \u001b[38;5;129;01mand\u001b[39;00m active_agent:\n\u001b[1;32m    258\u001b[0m \n\u001b[1;32m    259\u001b[0m     \u001b[38;5;66;03m# get completion with current history, agent\u001b[39;00m\n\u001b[0;32m--> 260\u001b[0m     completion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_chat_completion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mactive_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext_variables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext_variables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_override\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_override\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdebug\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdebug\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    268\u001b[0m     message \u001b[38;5;241m=\u001b[39m completion\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\n\u001b[1;32m    269\u001b[0m     debug_print(debug, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived completion:\u001b[39m\u001b[38;5;124m\"\u001b[39m, message)\n",
      "File \u001b[0;32m~/miniconda3/envs/semantic_research_engine/lib/python3.10/site-packages/swarm/core.py:69\u001b[0m, in \u001b[0;36mSwarm.get_chat_completion\u001b[0;34m(self, agent, history, context_variables, model_override, stream, debug)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tools:\n\u001b[1;32m     67\u001b[0m     create_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mparallel_tool_calls\n\u001b[0;32m---> 69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcreate_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/semantic_research_engine/lib/python3.10/site-packages/openai/_utils/_utils.py:275\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 275\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/semantic_research_engine/lib/python3.10/site-packages/openai/resources/chat/completions.py:829\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    826\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    827\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m    828\u001b[0m     validate_response_format(response_format)\n\u001b[0;32m--> 829\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    830\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    832\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    833\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    834\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    835\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maudio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    836\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    837\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    838\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    840\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodalities\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    850\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/semantic_research_engine/lib/python3.10/site-packages/openai/_base_client.py:1278\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1264\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1265\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1266\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1273\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1274\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1275\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1276\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1277\u001b[0m     )\n\u001b[0;32m-> 1278\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/semantic_research_engine/lib/python3.10/site-packages/openai/_base_client.py:955\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    953\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 955\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/semantic_research_engine/lib/python3.10/site-packages/openai/_base_client.py:991\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m    988\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSending HTTP Request: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, request\u001b[38;5;241m.\u001b[39mmethod, request\u001b[38;5;241m.\u001b[39murl)\n\u001b[1;32m    990\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 991\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    992\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    993\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    994\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    995\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    996\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mTimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    997\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered httpx.TimeoutException\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/semantic_research_engine/lib/python3.10/site-packages/httpx/_client.py:926\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_timeout(request)\n\u001b[1;32m    924\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m--> 926\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    929\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    931\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    933\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[0;32m~/miniconda3/envs/semantic_research_engine/lib/python3.10/site-packages/httpx/_client.py:954\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    951\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[1;32m    953\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 954\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    959\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    960\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/semantic_research_engine/lib/python3.10/site-packages/httpx/_client.py:991\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    988\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    989\u001b[0m     hook(request)\n\u001b[0;32m--> 991\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    992\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    993\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/miniconda3/envs/semantic_research_engine/lib/python3.10/site-packages/httpx/_client.py:1027\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1023\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1024\u001b[0m     )\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[0;32m-> 1027\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[1;32m   1031\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[0;32m~/miniconda3/envs/semantic_research_engine/lib/python3.10/site-packages/httpx/_transports/default.py:236\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    223\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[1;32m    224\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    225\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    233\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    234\u001b[0m )\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 236\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[1;32m    241\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[1;32m    242\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    243\u001b[0m     stream\u001b[38;5;241m=\u001b[39mResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[1;32m    244\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    245\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/semantic_research_engine/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:256\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    253\u001b[0m         closing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_requests_to_connections()\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[0;32m--> 256\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n",
      "File \u001b[0;32m~/miniconda3/envs/semantic_research_engine/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:236\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    232\u001b[0m connection \u001b[38;5;241m=\u001b[39m pool_request\u001b[38;5;241m.\u001b[39mwait_for_connection(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[0;32m--> 236\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[1;32m    244\u001b[0m     pool_request\u001b[38;5;241m.\u001b[39mclear_connection()\n",
      "File \u001b[0;32m~/miniconda3/envs/semantic_research_engine/lib/python3.10/site-packages/httpcore/_sync/connection.py:103\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/semantic_research_engine/lib/python3.10/site-packages/httpcore/_sync/http11.py:136\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_closed\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    135\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[0;32m--> 136\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m~/miniconda3/envs/semantic_research_engine/lib/python3.10/site-packages/httpcore/_sync/http11.py:106\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[1;32m     99\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    100\u001b[0m     (\n\u001b[1;32m    101\u001b[0m         http_version,\n\u001b[1;32m    102\u001b[0m         status,\n\u001b[1;32m    103\u001b[0m         reason_phrase,\n\u001b[1;32m    104\u001b[0m         headers,\n\u001b[1;32m    105\u001b[0m         trailing_data,\n\u001b[0;32m--> 106\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    108\u001b[0m         http_version,\n\u001b[1;32m    109\u001b[0m         status,\n\u001b[1;32m    110\u001b[0m         reason_phrase,\n\u001b[1;32m    111\u001b[0m         headers,\n\u001b[1;32m    112\u001b[0m     )\n\u001b[1;32m    114\u001b[0m network_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\n",
      "File \u001b[0;32m~/miniconda3/envs/semantic_research_engine/lib/python3.10/site-packages/httpcore/_sync/http11.py:177\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    174\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 177\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n\u001b[1;32m    179\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/semantic_research_engine/lib/python3.10/site-packages/httpcore/_sync/http11.py:217\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    214\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[0;32m--> 217\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[0;32m~/miniconda3/envs/semantic_research_engine/lib/python3.10/site-packages/httpcore/_backends/sync.py:128\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[0;32m--> 128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "topic = \"ChunkRag\"\n",
    "run_arxiv_paper_summary_workflow(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkRag\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6409/2044149504.py:20: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  for result in search.results():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the details for ChunkRAG:\n",
      "\n",
      "- **Title:** ChunkRAG: Novel LLM-Chunk Filtering Method for RAG Systems\n",
      "- **Date:** 2024-10-25\n",
      "- **URL:** [http://arxiv.org/pdf/2410.19572v4](http://arxiv.org/pdf/2410.19572v4)\n",
      "- **Summary:**\n",
      "Retrieval-Augmented Generation (RAG) systems using large language models often generate inaccurate responses due to the retrieval of irrelevant or loosely related information. Existing methods, which operate at the document level, fail to effectively filter out such content.\n",
      "\n",
      "ChunkRAG is a proposed method in this paper that enhances RAG systems by filtering retrieved information at the chunk level. The framework uses semantic chunking to divide documents into coherent sections and LLM-based relevance scoring to assess each chunk's alignment with the user's query. By filtering out less pertinent chunks before the generation phase, it reduces hallucinations and improves factual accuracy.\n",
      "\n",
      "Experiments have shown that this method outperforms existing RAG models on tasks requiring precise information retrieval. This advancement enhances the reliability of RAG systems, making them particularly beneficial for applications such as fact-checking or multi-hop reasoning.\n",
      "- **Type:** [arxiv.org/pdf/2410.19572v4 (PDF)](http://arxiv.org/pdf/2410.19572v4)\n"
     ]
    }
   ],
   "source": [
    "paper_details_response = client.run(agent=url_agent,\n",
    "        messages=[{\"role\":\"user\",\"content\":f\"Get me the details for {topic}\"}])\n",
    "text = paper_details_response.messages[-1]['content']\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The URL for the content you provided is:\n",
      "\n",
      "[http://arxiv.org/pdf/2410.19572v4](http://arxiv.org/pdf/2410.19572v4)\n"
     ]
    }
   ],
   "source": [
    "# Step 2 Extract the URL from the search results\n",
    "url_response = client.run(agent=extract_url,\n",
    "messages=[{\"role\":\"user\",\"content\":f\"Get me the URL from the content{text}\"}])\n",
    "#\n",
    "text2 = url_response.messages[-1]['content']\n",
    "print(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regex pattern to find URLs\n",
    "url_pattern = r'\\((https?://[^\\s)]+)\\)'\n",
    "\n",
    "# Find all unique URLs in the text\n",
    "urls = set(re.findall(url_pattern, text2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'http://arxiv.org/pdf/2410.19572v4'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "urls :{'http://arxiv.org/pdf/2410.19572v4'}\n",
      "['http://arxiv.org/pdf/2410.19572v4']\n"
     ]
    }
   ],
   "source": [
    "print(f\"urls :{urls}\")\n",
    "print(list(urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content :ChunkRAG: Novel LLM-Chunk Filtering Method for RAG Systems\n",
      "\n",
      "Ishneet Sukhvinder Singh*\n",
      "\n",
      "Ritvik Aggarwal*\n",
      "\n",
      "Aslihan Akalin\n",
      "\n",
      "Kevin Zhu\n",
      "\n",
      "Ibrahim Allahverdiyev\n",
      "Sean O’Brien\n",
      "\n",
      "Muhammad Taha\n",
      "\n",
      "Algoverse AI Research\n",
      "asli@algoverse.us, kevin@algoverse.us, seobrien@ucsd.edu\n",
      "\n",
      "4\n",
      "2\n",
      "0\n",
      "2\n",
      "\n",
      "v\n",
      "o\n",
      "N\n",
      "9\n",
      "1\n",
      "\n",
      "]\n",
      "L\n",
      "C\n",
      ".\n",
      "s\n",
      "c\n",
      "[\n",
      "\n",
      "4\n",
      "v\n",
      "2\n",
      "7\n",
      "5\n",
      "9\n",
      "1\n",
      ".\n",
      "0\n",
      "1\n",
      "4\n",
      "2\n",
      ":\n",
      "v\n",
      "i\n",
      "X\n",
      "r\n",
      "a\n",
      "\n",
      "Abstract\n",
      "\n",
      "Retrieval-Augmented Generation (RAG) sys-\n",
      "tems using large language models (LLMs) of-\n",
      "ten generate inaccurate responses due to the\n",
      "retrieval of irrelevant or loosely related infor-\n",
      "mation. Existing methods, which operate at the\n",
      "document level, fail to effectively filter out such\n",
      "content. We propose LLM-driven chunk filter-\n",
      "ing, ChunkRAG, a framework that enhances\n",
      "RAG systems by evaluating and filtering re-\n",
      "trieved information at the chunk level, where\n",
      "a \"chunk\" represents a smaller, coherent sec-\n",
      "tion of a document. Our approach employs\n",
      "semantic chunking to divide documents into\n",
      "coherent sections and utilizes LLM-based rel-\n",
      "evance scoring to assess each chunk’s align-\n",
      "ment with the user’s query. By filtering out less\n",
      "pertinent chunks before the generation phase,\n",
      "we significantly reduce hallucinations and im-\n",
      "prove factual accuracy. Experiments show that\n",
      "our method outperforms existing RAG models,\n",
      "achieving higher accuracy on tasks requiring\n",
      "precise information retrieval. This advance-\n",
      "ment enhances the reliability of RAG systems,\n",
      "making them particularly beneficial for applica-\n",
      "tions like fact-checking and multi-hop reason-\n",
      "ing.\n",
      "\n",
      "1\n",
      "\n",
      "Introduction\n",
      "\n",
      "Large language models (LLMs) have made sig-\n",
      "nificant strides in the development of retrieval-\n",
      "augmented generation (RAG) systems, which com-\n",
      "bine retrieval mechanisms with powerful language\n",
      "models to produce responses based on external\n",
      "knowledge. However, despite these advancements,\n",
      "a persistent issue remains: the retrieval of irrelevant\n",
      "or weakly related information during the document-\n",
      "fetching process. Current retrieval techniques, in-\n",
      "cluding reranking and query rewriting, not only fail\n",
      "to filter out lots of irrelevant chunks of informa-\n",
      "tion in the retrieved documents but also lead to a\n",
      "series of problems with factual inaccuracies, irrel-\n",
      "evance, and hallucinations in the responses gener-\n",
      "\n",
      "1\n",
      "\n",
      "Figure 1: Comparison of Response Generation With\n",
      "and Without Chunk Filtering\n",
      "\n",
      "ated (Zhang and Others, 2023; Mallen et al., 2023).\n",
      "\n",
      "Traditionally, RAG systems retrieve large\n",
      "amounts of the text of entire documents or lengthy\n",
      "portions thereof, assuming that it is likely that these\n",
      "lengthy fragments will contain the relevant informa-\n",
      "tion. Such systems very rarely examine the sections\n",
      "or paragraphs of the retrieved documents individ-\n",
      "ually and, therefore, there is a strong likelihood\n",
      "that irrelevant or only partially related information\n",
      "will flow into the generation stage. This is fur-\n",
      "ther worsened by the fact that language models\n",
      "generate fluent text without being able to verify\n",
      "the information they use for generation. Relevant\n",
      "or misleading chunks distort the outcome of such\n",
      "models severely, reducing the system’s reliability,\n",
      "especially in critical tasks such as open-domain\n",
      "question answering and multi-hop reasoning (Ji\n",
      "et al., 2023; Min et al., 2023).\n",
      "\n",
      "Fig. 1: The figure shows that without chunk\n",
      "filtering (top), irrelevant information like other\n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\fFrench cities is included in the response. The\n",
      "LLM-driven chunk filtering (bottom), however, re-\n",
      "moves unnecessary content, delivering the precise\n",
      "answer, \"The capital of France is Paris.\" A few\n",
      "retrieval-related methods, Corrective RAG (CRAG)\n",
      "and Self-RAG, have attempted to overcome these\n",
      "hurdles by refining the retrieval process. CRAG fo-\n",
      "cuses on retrieving \"corrections\" after errors occur\n",
      "in retrieval, whereas Self-RAG introduces a self-\n",
      "reflection mechanism during the generation stage\n",
      "to minimize inaccuracies. Both of these processes\n",
      "operate at the document level and lack sufficient fil-\n",
      "tering for individual retrieved chunks of text. This\n",
      "document-level approach enhances the overall rele-\n",
      "vance of the retrieval but does not prevent irrelevant\n",
      "chunks from being included in the generated re-\n",
      "sponse (Shi et al., 2023). Without control over the\n",
      "granularity of the retrieved content, RAG systems\n",
      "remain vulnerable to incorporating undesirable or\n",
      "misleading information into their output, ultimately\n",
      "compromising performance.\n",
      "\n",
      "The solution to this challenge lies in the novel ap-\n",
      "proach: LLM-driven chunk filtering, ChunkRAG.\n",
      "Our method operates on a finer level of granularity\n",
      "than classical systems and, in fact, supports chunk-\n",
      "level filtering of retrieved information. Rather than\n",
      "judging entire documents to be relevant, our system\n",
      "goes both for the user query and individual chunks\n",
      "within retrieved documents. The large language\n",
      "model evaluates semantic relevance of each chunk\n",
      "with respect to the user’s query; this makes the sys-\n",
      "tem capable of filtering out irrelevant or weakly\n",
      "related chunks even before they get into the gener-\n",
      "ation stage. This chunk-level filtering in turn aims\n",
      "to enforce factual accuracy on the final answer by\n",
      "drawing only the most relevant information on the\n",
      "generation. This approach is particularly promising\n",
      "for knowledge-intensive tasks, such as multi-hop\n",
      "reasoning and fact-checking: precision is the ulti-\n",
      "mate prize here. That is, in tasks where accuracy is\n",
      "paramount, our approach stands best (Piktus et al.,\n",
      "2021; Rony et al., 2022).\n",
      "\n",
      "2 Related Works\n",
      "\n",
      "Addressing Hallucinations in Large Language\n",
      "Models Large language models (LLMs) have made\n",
      "significant strides in understanding instructions and\n",
      "generating coherent text (Bang et al., 2023; Qin et\n",
      "al., 2023; Zhong et al., 2023). However, they still\n",
      "grapple with the issue of hallucinations, where the\n",
      "\n",
      "models produce outputs that are incorrect or non-\n",
      "sensical. Research indicates that the activation of\n",
      "outdated or erroneous knowledge contributes to this\n",
      "problem (Tonmoy et al., 2024; Zhang et al., 2023b;\n",
      "Shuster et al., 2021). Factors such as reliance on\n",
      "large, unregulated datasets, a low proportion of\n",
      "high-quality training samples, and suboptimal data\n",
      "distribution within the input space exacerbate these\n",
      "challenges. The absence of precise and accurate\n",
      "knowledge often leads to misleading or inaccurate\n",
      "outputs, severely impacting user experience in prac-\n",
      "tical applications.\n",
      "\n",
      "Retrieval-Augmented Generation Retrieval-\n",
      "Augmented Generation (RAG) has been proposed\n",
      "as an effective strategy to mitigate hallucinations\n",
      "(Lewis et al., 2020; Guu et al., 2020). By augment-\n",
      "ing input queries with retrieved documents from\n",
      "specific corpora like Wikipedia, RAG provides ad-\n",
      "ditional knowledge that enhances the performance\n",
      "of LLMs, especially in tasks that are knowledge-\n",
      "intensive. This approach involves using informa-\n",
      "tion retrieval systems to supply relevant documents\n",
      "to the generative models. Early implementations\n",
      "employed either sparse or dense retrievers preced-\n",
      "ing a pretrained language model focused on re-\n",
      "sponse generation. However, these methods often\n",
      "overlook a critical question: What happens if the re-\n",
      "trieval process fails or retrieves irrelevant informa-\n",
      "tion? Irrelevant documents can worsen the factual\n",
      "inaccuracies of the model’s output, counteracting\n",
      "the benefits of retrieval augmentation.\n",
      "\n",
      "Advancements in Retrieval Techniques Re-\n",
      "cent developments have aimed to refine RAG meth-\n",
      "ods to address these limitations (Zhang et al.,\n",
      "2024; Kim et al., 2024; Wang et al., 2024; Liu\n",
      "et al., 2024). Recognizing that retrieval is not al-\n",
      "ways necessary—and can sometimes decrease accu-\n",
      "racy—approaches like SelfRAG (Asai et al., 2024)\n",
      "incorporate mechanisms to selectively decide when\n",
      "to retrieve information, using a critic model for\n",
      "this purpose. CRAG (Your et al., 2024) is a re-\n",
      "cent approach that augments standard RAG with\n",
      "corrective strategies to improve retrieval quality by\n",
      "addressing low-quality retrieval results. Yoran et\n",
      "al. (2024) introduced a Natural Language Infer-\n",
      "ence (NLI) model to detect and filter out irrelevant\n",
      "contexts, enhancing the robustness of the system.\n",
      "SAIL (Luo et al., 2023) fine-tunes models to insert\n",
      "retrieved documents before processing instructions,\n",
      "improving the integration of external knowledge.\n",
      "Toolformer (Schick et al., 2023) pretrains models\n",
      "to interact with APIs like Wikipedia, enabling dy-\n",
      "\n",
      "2\n",
      "\n",
      "\fnamic access to information. In scenarios involving\n",
      "long-form text generation, where external knowl-\n",
      "edge may be needed multiple times, determining\n",
      "the optimal timing for retrieval becomes crucial.\n",
      "Jiang et al. (2023) propose anticipating future con-\n",
      "tent needs to decide when and what information to\n",
      "retrieve during the generation process.\n",
      "\n",
      "Redundancy in retrieved information can dimin-\n",
      "ish the effectiveness of RAG models by introduc-\n",
      "ing repetitive or irrelevant data, which hampers the\n",
      "model’s ability to generate coherent and unique\n",
      "responses. One prevalent approach to mitigating\n",
      "redundancy involves the use of cosine similarity\n",
      "to evaluate and remove duplicate or overly similar\n",
      "content from the retrieved documents.\n",
      "\n",
      "Cosine Similarity in Redundancy Removal\n",
      "Cosine similarity measures the cosine of the angle\n",
      "between two non-zero vectors of an inner product\n",
      "space, which quantifies the similarity between the\n",
      "In\n",
      "two vectors irrespective of their magnitude.\n",
      "the context of RAG, it is employed to compare\n",
      "textual embeddings of retrieved chunks to identify\n",
      "and eliminate redundant content, enhancing the\n",
      "diversity of the information available for generation\n",
      "(Liu et al., 2023).\n",
      "\n",
      "Multi-Meta-RAG for Multi-Hop Queries Ad-\n",
      "dressing the challenges of multi-hop queries, Multi-\n",
      "Meta-RAG introduces a database filtering mecha-\n",
      "nism using metadata extracted by large language\n",
      "models (LLMs). By incorporating LLM-extracted\n",
      "metadata, this approach filters databases to retrieve\n",
      "more relevant documents that contribute to answer-\n",
      "ing complex queries requiring reasoning over multi-\n",
      "ple pieces of information (Smith et al., 2023). This\n",
      "method reduces redundancy by ensuring that only\n",
      "pertinent documents are considered, thereby im-\n",
      "proving the coherence of the generated responses.\n",
      "Query Rewriting for Enhanced Retrieval A\n",
      "\"Rewrite-Retrieve-Read\" framework to bridge the\n",
      "gap between input text and the necessary retrieval\n",
      "knowledge is proposed (Johnson and Lee, 2023).\n",
      "A trainable query rewriter adapts queries using re-\n",
      "inforcement learning based on feedback from the\n",
      "LLM’s performance. This approach enhances re-\n",
      "trieval accuracy by reformulating queries to better\n",
      "align with relevant documents, thus minimizing the\n",
      "retrieval of redundant or irrelevant information.\n",
      "\n",
      "We introduce a new model, ChunkRAG, that\n",
      "emphasizes a chunking strategy aimed at further\n",
      "reducing redundancy and improving the effective-\n",
      "ness of RAG models. Compared with recent studies\n",
      "(Schick et al., 2023; Luo et al., 2023; Asai et al.,\n",
      "\n",
      "2024, Your et al., 2024), our approach involves seg-\n",
      "menting documents into semantically coherent and\n",
      "non-overlapping chunks that are more aligned with\n",
      "the specific information needs of the query.\n",
      "\n",
      "3 Methodology\n",
      "\n",
      "The core objective of this work is to mitigate irrele-\n",
      "vance and hallucinations in the responses generated\n",
      "by Retrieval-Augmented Generation (RAG) sys-\n",
      "tems, using a novel, fine-grained filtering mecha-\n",
      "nism that rigorously evaluates the relevance of each\n",
      "chunk of retrieved information before integrating\n",
      "it into the response generation phase. Our pro-\n",
      "posed methodology follows a two-stage approach:\n",
      "semantic chunking and advanced filtering to refine\n",
      "retrieval results. Each stage is designed to enhance\n",
      "the system’s precision and reliability in leveraging\n",
      "retrieved knowledge. Below, we detail the compo-\n",
      "nents of our proposed method.\n",
      "\n",
      "Figure 2: ChunkRAG Methodology for Enhanced\n",
      "Retrieval and Filtering\n",
      "\n",
      "Semantic Chunking\n",
      "\n",
      "Semantic chunking serves as the foundational step\n",
      "of our methodology, transforming the input docu-\n",
      "ment into semantically meaningful units to facili-\n",
      "tate effective retrieval and evaluation. This stage\n",
      "involves three sub-processes:\n",
      "\n",
      "• Input Preparation: We begin by tokenizing\n",
      "a document D into sentences using NLTK’s\n",
      "\n",
      "3\n",
      "\n",
      "\fsent_tokenize function. Each sentence is\n",
      "then assigned an embedding vector, generated\n",
      "using a pre-trained embedding model (e.g.,\n",
      "text-embedding-3-small).\n",
      "\n",
      "• Chunk Formation: Consecutive sentences are\n",
      "grouped into chunks based on their seman-\n",
      "tic similarity, measured through cosine simi-\n",
      "larity. Specifically, if the similarity between\n",
      "consecutive sentences drops below a thresh-\n",
      "old (θ = 0.7), a new chunk is created. Each\n",
      "chunk is further constrained to be under 500\n",
      "characters to ensure efficiency during subse-\n",
      "quent stages.\n",
      "\n",
      "• Embedding Generation for Chunks:\n",
      "\n",
      "Each\n",
      "chunk is represented using the same pre-\n",
      "trained embedding model as above. The\n",
      "resultant chunk embeddings are stored in a\n",
      "vector database to facilitate efficient retrieval\n",
      "during the query phase.\n",
      "\n",
      "Hybrid Retrieval and Advanced Filtering\n",
      "\n",
      "In the retrieval and filtering phase, we integrate\n",
      "conventional RAG components with advanced fine-\n",
      "tuning techniques to ensure robust and high-quality\n",
      "retrieval. The hybrid retrieval and filtering stage is\n",
      "detailed below:\n",
      "\n",
      "• Retriever Initialization and Query Rewriting:\n",
      "We initialize a retriever capable of comparing\n",
      "user queries against the chunk embeddings.\n",
      "To enhance query efficacy, we apply a query\n",
      "rewriting step using GPT-4omini, ensuring\n",
      "that the queries are well-matched to the stored\n",
      "embeddings. This ensures better recall and\n",
      "precision in the retrieval process.\n",
      "\n",
      "• Initial Filtering: Retrieved chunks are initially\n",
      "filtered using a combination of TF-IDF scor-\n",
      "ing and cosine similarity. Chunks with high\n",
      "redundancy (similarity > 0.9) are eliminated.\n",
      "The remaining chunks are sorted based on\n",
      "their similarity to the rewritten query.\n",
      "\n",
      "• Relevance Scoring and Thresholding: To fur-\n",
      "ther refine relevance, each chunk is assigned\n",
      "an initial relevance score by a large lan-\n",
      "guage model (LLM). These scores are refined\n",
      "through self-reflection and a critic model,\n",
      "which adjusts the scores based on domain-\n",
      "specific heuristics. A final dynamic threshold\n",
      "is set by analyzing the score distribution, and\n",
      "\n",
      "only chunks surpassing this threshold are re-\n",
      "tained.\n",
      "\n",
      "• Hybrid Retrieval Strategy: To maximize re-\n",
      "trieval effectiveness, we employ a dual\n",
      "retrieval strategy combining BM25 and\n",
      "LLM-based retrieval methods.\n",
      "The en-\n",
      "semble approach uses equal weighting (0.5\n",
      "each) to balance keyword and semantic re-\n",
      "trieval.\n",
      "Furthermore, Cohere’s reranking\n",
      "model (rerank-englishv3.0) is used to rank\n",
      "the retrieved chunks, addressing the Lost in\n",
      "the middle problem by enhancing the rele-\n",
      "vance of central context that might otherwise\n",
      "be deprioritized.\n",
      "\n",
      "Response Generation and Evaluation\n",
      "\n",
      "After filtering, the remaining chunks are used as\n",
      "context to generate the final response. The steps\n",
      "include:\n",
      "\n",
      "• Response Generation: An LLM generates\n",
      "a response based on the filtered context\n",
      "chunks. During generation, strict constraints\n",
      "ensure that only retrieved information is used,\n",
      "thereby minimizing the risk of hallucinations.\n",
      "\n",
      "• Evaluation: The generated responses are\n",
      "evaluated for accuracy against a set of pre-\n",
      "validated answers.\n",
      "\n",
      "Our methodology, combining semantic chunking\n",
      "with advanced retrieval and filtering mechanisms,\n",
      "significantly enhances the quality of responses pro-\n",
      "duced by RAG systems, ensuring both relevance\n",
      "and correctness of the generated content. The em-\n",
      "pirical results, as described in subsequent sections,\n",
      "demonstrate the effectiveness of our approach in\n",
      "various retrieval and generation scenarios.\n",
      "\n",
      "4 Experiments\n",
      "\n",
      "We conducted experiments to extensively demon-\n",
      "strate ChunkRAG’s adaptability and its poten-\n",
      "tial for generalizability across various generation\n",
      "tasks. However, due to computational resource con-\n",
      "straints, our evaluation was primarily focused on\n",
      "the PopQA dataset.\n",
      "\n",
      "4.1 Tasks and Datasets\n",
      "\n",
      "ChunkRAG was evaluated on the PopQA dataset,\n",
      "which serves as the cornerstone of our experimen-\n",
      "tal analysis. PopQA (Mallen et al., 2023) is a\n",
      "\n",
      "4\n",
      "\n",
      "\fbenchmark dataset designed for short-form ques-\n",
      "tion answering. It comprises a diverse set of ques-\n",
      "tions that require concise and accurate responses,\n",
      "making it an ideal testbed for assessing the perfor-\n",
      "mance of retrieval-augmented generation models\n",
      "like ChunkRAG.\n",
      "\n",
      "To measure the effectiveness of ChunkRAG, we\n",
      "adopted accuracy as the evaluation metric, consis-\n",
      "tent with prior studies. This metric aligns with the\n",
      "conventions used in the evaluation of PopQA, en-\n",
      "suring that our results are comparable to existing\n",
      "research.\n",
      "\n",
      "While our current experiments are limited to\n",
      "PopQA, ChunkRAG is architected with scalabil-\n",
      "ity in mind. Future evaluations may extend to ad-\n",
      "ditional datasets such as Biography (Min et al.,\n",
      "2023) for long-form generation, PubHealth (Zhang\n",
      "et al., 2023) for true/false question answering, and\n",
      "Arc-Challenge (Bhakthavatsalam et al., 2021) for\n",
      "multiple-choice questions. These extensions will\n",
      "further validate ChunkRAG’s versatility across dif-\n",
      "ferent types of generation tasks, contingent upon\n",
      "the availability of computational resources.\n",
      "\n",
      "4.2 Baselines\n",
      "\n",
      "4.2.1 Baselines Without Retrieval\n",
      "We first evaluated several large language models\n",
      "(LLMs) that do not incorporate retrieval mecha-\n",
      "nisms. Among the public LLMs, we included\n",
      "LLaMA2-7B and LLaMA2-13B (Touvron et al.,\n",
      "2023), known for their versatility across diverse nat-\n",
      "ural language processing (NLP) tasks, and Alpaca-\n",
      "7B and Alpaca-13B (Dubois et al., 2023), which are\n",
      "instruction-tuned models optimized for effectively\n",
      "following user prompts. Additionally, we assessed\n",
      "CoVE65B (Dhuliawala et al., 2024), which intro-\n",
      "duces iterative engineering techniques aimed at en-\n",
      "hancing the factual accuracy of generated content.\n",
      "For proprietary models, we included LLaMA2-\n",
      "chat13B, a conversational variant of LLaMA2 tai-\n",
      "lored for dialogue-based applications, and Chat-\n",
      "GPT, OpenAI’s proprietary conversational agent\n",
      "renowned for its robust language understanding\n",
      "and generation capabilities.\n",
      "\n",
      "4.2.2 Baselines With Retrieval\n",
      "Standard Retrieval-Augmented Generation\n",
      "(RAG): To establish a baseline for retrieval-\n",
      "augmented methods, we evaluated standard RAG\n",
      "approaches. Specifically, we employed Standard\n",
      "RAG (Lewis et al., 2020), which utilizes a retriever\n",
      "to fetch relevant documents based on the input\n",
      "\n",
      "query, subsequently feeding these documents into\n",
      "the language model to generate responses. For\n",
      "consistency, we utilized the same retriever mech-\n",
      "anism as ChunkRAG to ensure a fair compari-\n",
      "son. In addition to Standard RAG, we evaluated\n",
      "instruction-tuned LLMs with standard RAG, in-\n",
      "cluding LLaMA2-7B, LLaMA2-13B, and Alpaca-\n",
      "7B, Alpaca-13B, to assess the impact of instruction\n",
      "tuning in conjunction with retrieval augmentation.\n",
      "Advanced Retrieval-Augmented Generation:\n",
      "To benchmark ChunkRAG against more sophisti-\n",
      "cated RAG-based methods, we included advanced\n",
      "models that incorporate additional strategies to en-\n",
      "hance performance. SAIL (Luo et al., 2023) en-\n",
      "hances standard RAG by instruction-tuning the lan-\n",
      "guage model on Alpaca instruction-tuning data, in-\n",
      "serting top retrieved documents before the instruc-\n",
      "tions to provide contextual information. Self-RAG\n",
      "(Asai et al., 2024) further refines RAG by incorpo-\n",
      "rating reflection tokens labeled by GPT-4 within\n",
      "the instruction-tuning data, enabling the model to\n",
      "better utilize retrieved information. Additionally,\n",
      "we considered CRAG (Your et al., 2024), a recent\n",
      "approach that augments standard RAG with correc-\n",
      "tive strategies to improve retrieval quality by ad-\n",
      "dressing low-quality retrieval results. CRAG serves\n",
      "as a direct comparison to our proposed ChunkRAG,\n",
      "highlighting the effectiveness of our chunk filter-\n",
      "ing mechanism in enhancing retrieval-augmented\n",
      "generation. Furthermore, we evaluated retrieval-\n",
      "augmented baselines with private data, including\n",
      "Ret-ChatGPT and RetLLaMA-chat, which inte-\n",
      "grate retrieval mechanisms with ChatGPT and the\n",
      "conversational variant of LLaMA2, respectively.\n",
      "\n",
      "5 Analysis\n",
      "\n",
      "In this section, we evaluate the performance of\n",
      "ChunkRAG against existing retrieval-augmented\n",
      "generation (RAG) methods. We present an analysis\n",
      "based on empirical results obtained from standard\n",
      "benchmarks.\n",
      "\n",
      "5.1 Evaluation Metrics\n",
      "\n",
      "We used accuracy as the primary evaluation metric,\n",
      "calculated as the percentage of generated responses\n",
      "that match the ground-truth answers.\n",
      "\n",
      "5.2 Comparison and Impact\n",
      "\n",
      "As depicted in Table 1, our method achieved an\n",
      "accuracy of 64.9, substantially outperforming all\n",
      "baselines in the same category. Notably, compared\n",
      "\n",
      "5\n",
      "\n",
      "\fTable 1: Performance Comparison Across Methods\n",
      "(PopQA Accuracy Only)\n",
      "\n",
      "Method\n",
      "\n",
      "PopQA (Accuracy)\n",
      "\n",
      "LLMs trained with proprietary data\n",
      "\n",
      "LLaMA2-C_13B\n",
      "Ret-LLaMA2-C_13B\n",
      "ChatGPT\n",
      "Ret-ChatGPT\n",
      "\n",
      "20.0\n",
      "51.8\n",
      "29.3\n",
      "50.8\n",
      "Baselines without retrieval\n",
      "14.7\n",
      "23.6\n",
      "14.7\n",
      "14.3\n",
      "-\n",
      "Baselines with retrieval\n",
      "\n",
      "LLaMA2_7B\n",
      "Alpaca_7B\n",
      "LLaMA2_13B\n",
      "Alpaca_13B\n",
      "CoVE_65B\n",
      "\n",
      "LLaMA2_7B\n",
      "Alpaca_7B\n",
      "SAIL\n",
      "LLaMA2_13B\n",
      "Alpaca_13B\n",
      "\n",
      "LLaMA2-hf_7B\n",
      "\n",
      "RAG\n",
      "CRAG\n",
      "Self-RAG\n",
      "Self-CRAG\n",
      "ChunkRAG\n",
      "\n",
      "38.2\n",
      "46.7\n",
      "-\n",
      "45.7\n",
      "46.1\n",
      "\n",
      "50.5\n",
      "54.9\n",
      "50.5\n",
      "49.0\n",
      "64.9\n",
      "\n",
      "to the closest baseline, CRAG (54.9 accuracy), our\n",
      "method exhibits a performance gain of 10 percent-\n",
      "age points.\n",
      "\n",
      "While a 10 percentage point increase may seem\n",
      "incremental, it translates into an exponential im-\n",
      "provement in output effectiveness in practical ap-\n",
      "plications. This is particularly evident when consid-\n",
      "ering the error rates and their impact on the overall\n",
      "user experience.In applications requiring multi-hop\n",
      "reasoning or sequential decision-making, errors\n",
      "can compound exponentially. This exponential\n",
      "improvement is especially important in complex\n",
      "tasks where each additional step compounds the\n",
      "risk of error, namely relevant to OpenAI’s advanced\n",
      "models such as o1 where the language model uti-\n",
      "lizes multi-hop reasoning, relying on spending time\n",
      "\"thinking\" before it answers, making it more effi-\n",
      "cient in complex reasoning tasks, science and pro-\n",
      "gramming.\n",
      "\n",
      "5.3 Observations and Insights\n",
      "\n",
      "The notable improvement attained with our tech-\n",
      "nique is mainly due to chunk-level filtering and\n",
      "fine-grained relevance assessment. We divided\n",
      "the text into semantically meaningful chunks,\n",
      "which reduced the generation of irrelevant or\n",
      "In processing the\n",
      "weakly related information.\n",
      "chunk filtering’s contextually relevant data, the gen-\n",
      "eration of factually accurate and coherent responses\n",
      "was significantly enhanced.\n",
      "\n",
      "6\n",
      "\n",
      "Moreover,\n",
      "\n",
      "the self-reflective LLM scoring\n",
      "method, in which the model grades itself and then\n",
      "changes accordingly, led to a significant decrease\n",
      "in retrieval errors. Unlike regular retrieval methods\n",
      "that do not have a filtering mechanism at the doc-\n",
      "ument section level, our method can extract more\n",
      "meaningful and relevant information that directly\n",
      "affects the reliability of the generated responses.\n",
      "\n",
      "5.4 Future Work\n",
      "\n",
      "In our present studies, we have only tested PopQA\n",
      "but the design of ChunkRAG is for scalability\n",
      "purposes. In the upcoming assessments, we will\n",
      "also introduce new datasets including Biography\n",
      "for long-form generation, PubHealth for true/false\n",
      "questions, and Arc-Challenge for multiple-choice\n",
      "questions. The implementation of these trials will\n",
      "thus reinforce the evidence of ChunkRAG’s versa-\n",
      "tility and adaptability to different types of genera-\n",
      "tion tasks, although this will be conditional on the\n",
      "availability of computing resources.\n",
      "\n",
      "6 Conclusion\n",
      "\n",
      "In this paper, we introduced ChunkRAG, a novel\n",
      "LLM-driven chunk filtering approach aimed at im-\n",
      "proving the precision and factuality of retrieval-\n",
      "augmented generation systems. In our experiments,\n",
      "which were conducted on the PopQA dataset,\n",
      "ChunkRAG has clearly demonstrated superiority\n",
      "over existing baselines, and thus has achieved a\n",
      "significant performance boost of 10 percentage\n",
      "points, which was higher than the closest bench-\n",
      "mark, CRAG. The chunk-level filtering technique\n",
      "guaranteed that only the relevant and contextually\n",
      "correct information was included during the re-\n",
      "sponse generation, resulting in better reliability\n",
      "and accuracy of generated answers. This method is\n",
      "particularly useful for applications that require im-\n",
      "mense amounts of facts, such as multi-hop reason-\n",
      "ing and decision-making that involve many interde-\n",
      "pendent parameters. We believe that ChunkRAG\n",
      "is a big step towards solving the problems of ir-\n",
      "relevant or hallucinated material in LLM-based\n",
      "retrieval systems.\n",
      "\n",
      "7 Limitations\n",
      "\n",
      "ChunkRAG, in spite of its benefits, has a number\n",
      "of drawbacks that need to be taken into account.\n",
      "Firstly, the method relies heavily on the effective-\n",
      "ness of chunk segmentation and the quality of the\n",
      "embeddings used for chunk relevance assessment.\n",
      "\n",
      "\fMistakes in the primary division can create irrel-\n",
      "evant data that will decrease the quality of the re-\n",
      "sponse. Secondly, the costs from the multi-level\n",
      "score—integrating both LLM and critic LLM evalu-\n",
      "ations at the initial level—can be high, particularly\n",
      "during the scaling of the method to larger datasets\n",
      "or the deployment of it in real-time systems. Addi-\n",
      "tionally, while ChunkRAG demonstrated positive\n",
      "outcomes in the use of the PopQA dataset, the\n",
      "verifiability of its use in other domains and the per-\n",
      "formance when operating through long-form gen-\n",
      "eration tasks has not been thoroughly analyzed due\n",
      "to resource limitations. Future studies should con-\n",
      "centrate on the optimization of the computational\n",
      "efficiency of ChunkRAG and its evaluation over\n",
      "diverse datasets and in real-world applications.\n",
      "\n",
      "References\n",
      "\n",
      "A. Asai et al. 2024. Self-rag: Self-reflective retrieval-\n",
      "augmented generation for knowledge-intensive tasks.\n",
      "In Proceedings of the Annual Meeting of the Associa-\n",
      "tion for Computational Linguistics (ACL).\n",
      "\n",
      "S. Min et al. 2023. Self-reflective mechanisms for im-\n",
      "proved retrieval-augmented generation. In Proceed-\n",
      "ings of the 61st Annual Meeting of the Association\n",
      "for Computational Linguistics (ACL).\n",
      "\n",
      "A. Piktus et al. 2021. The role of chunking in retrieval-\n",
      "augmented generation. In Proceedings of the Con-\n",
      "ference on Neural Information Processing Systems\n",
      "(NeurIPS).\n",
      "\n",
      "M. S. Rony et al. 2022. Fine-grained document retrieval\n",
      "for fact-checking tasks. In Proceedings of the 2022\n",
      "Conference on Empirical Methods in Natural Lan-\n",
      "guage Processing (EMNLP).\n",
      "\n",
      "Y. Shi et al. 2023. Corrective retrieval in retrieval-\n",
      "augmented generation systems. In Proceedings of\n",
      "the International Conference on Machine Learning\n",
      "(ICML).\n",
      "\n",
      "T. Smith et al. 2023. Multi-meta-rag for multi-hop\n",
      "queries using llm-extracted metadata. In Proceedings\n",
      "of the International Conference on Computational\n",
      "Linguistics (COLING).\n",
      "\n",
      "H. Touvron et al. 2023.\n",
      "\n",
      "large language models.\n",
      "\n",
      "ficient\n",
      "arXiv:2307.12345.\n",
      "\n",
      "Llama2: Open and ef-\n",
      "arXiv preprint\n",
      "\n",
      "S. Bhakthavatsalam et al. 2021. Multi-hop reasoning\n",
      "In Proceedings of the\n",
      "with graph-based retrieval.\n",
      "59th Annual Meeting of the Association for Compu-\n",
      "tational Linguistics (ACL).\n",
      "\n",
      "S. Your et al. 2024. Crag: Corrective retrieval-\n",
      "In Proceedings of the An-\n",
      "augmented generation.\n",
      "nual Meeting of the Association for Computational\n",
      "Linguistics (ACL).\n",
      "\n",
      "A. Zhang and Others. 2023. Another title of the paper.\n",
      "\n",
      "arXiv preprint arXiv:2302.56789.\n",
      "\n",
      "A. Zhang et al. 2023. Hallucination in large language\n",
      "models: A comprehensive survey. arXiv preprint\n",
      "arXiv:2301.12345.\n",
      "\n",
      "F. Dhuliawala et al. 2024. Cove65b: Enhancing fac-\n",
      "tual accuracy through iterative engineering. arXiv\n",
      "preprint arXiv:2401.12345.\n",
      "\n",
      "Y. Dubois et al. 2023.\n",
      "\n",
      "Instruction tuning for open-\n",
      "domain question answering. In Advances in Neural\n",
      "Information Processing Systems (NeurIPS).\n",
      "\n",
      "Z. Ji et al. 2023. Survey of hallucination in generative\n",
      "\n",
      "models. arXiv preprint arXiv:2302.02451.\n",
      "\n",
      "R. Johnson and T. Lee. 2023. Query rewriting for\n",
      "retrieval-augmented large language models. In Pro-\n",
      "ceedings of the International Conference on Machine\n",
      "Learning (ICML).\n",
      "\n",
      "P. Lewis et al. 2020. Retrieval-augmented generation\n",
      "for knowledge-intensive nlp tasks. In Advances in\n",
      "Neural Information Processing Systems, volume 33,\n",
      "pages 9459–9474.\n",
      "\n",
      "S. Liu et al. 2023. Redundancy removal in retrieval-\n",
      "augmented generation using cosine similarity.\n",
      "In\n",
      "Proceedings of the Conference on Empirical Methods\n",
      "in Natural Language Processing (EMNLP).\n",
      "\n",
      "H. Luo et al. 2023. Sail: Instruction tuning for enhanced\n",
      "\n",
      "retrieval-augmented generation.\n",
      "\n",
      "J. Mallen et al. 2023. Enhancing retrieval-augmented\n",
      "In Proceedings of\n",
      "generation with fact-checking.\n",
      "the Conference on Empirical Methods in Natural\n",
      "Language Processing (EMNLP).\n",
      "\n",
      "7\n",
      "\n",
      "\f\n"
     ]
    }
   ],
   "source": [
    "content = extract_content(list(urls)[0])\n",
    "print(f\"Content :{content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "semantic_research_engine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
